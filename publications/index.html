<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Publications | Xuan (Tan Zhi Xuan)</title> <meta name="author" content="Xuan (Tan Zhi Xuan)"> <meta name="description" content="Xuan's personal academic website. "> <meta name="keywords" content="academic-website, bayesian-modeling, ai-alignment, cognitive-science, probabilistic-programming"> <meta property="og:site_name" content="Xuan (Tan Zhi Xuan)"> <meta property="og:type" content="website"> <meta property="og:title" content="Xuan (Tan Zhi Xuan) | Publications"> <meta property="og:url" content="https://ztangent.github.io/publications/"> <meta property="og:description" content="Xuan's personal academic website. "> <meta property="og:image" content="https://ztangent.github.io/assets/img/preview.png"> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="Publications"> <meta name="twitter:description" content="Xuan's personal academic website. "> <meta name="twitter:image" content="https://ztangent.github.io/assets/img/preview.png"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css2?family=Arimo:ital,wght@0,400;0,500;0,600;0,700;1,400;1,500;1,600;1,700&amp;display=swap"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9C%A8&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://ztangent.github.io/publications/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Xuan </span>(Tan Zhi Xuan)</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item "> <a class="nav-link" href="/news/">News</a> </li> <li class="nav-item "> <a class="nav-link" href="/talks/">Talks</a> </li> <li class="nav-item "> <a class="nav-link" href="/recruiting/">Recruiting</a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">Publications<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Publications</h1> <p class="post-description"></p> </header> <article> <div class="publications"> <h2 class="year">2025</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#AAE4F0"><a href="https://transacl.org/" rel="external nofollow noopener" target="_blank">TACL</a></abbr></div> <div id="ying2025understanding" class="col-sm-8"> <div class="title">Understanding Epistemic Language with a Language-augmented Bayesian Theory of Mind</div> <div class="author"> Lance Ying, <em>Tan Zhi-Xuan</em>, Lionel Wong, Vikash Mansinghka, and Joshua B Tenenbaum</div> <div class="periodical"> <em>Transactions of the Association for Computational Linguistics</em> 2025 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2408.12022" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/2025-epistemic-language.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>How do people understand and evaluate claims about others’ beliefs, even though these beliefs cannot be directly observed? In this paper, we introduce a cognitive model of epistemic language interpretation, grounded in Bayesian inferences about other agents’ goals, beliefs, and intentions: a language-augmented Bayesian theory-of-mind (LaBToM). By translating natural language into an epistemic “language-of-thought”, then evaluating these translations against the inferences produced by inverting a probabilistic generative model of rational action and perception, LaBToM captures graded plausibility judgments about epistemic claims. We validate our model in an experiment where participants watch an agent navigate a maze to find keys hidden in boxes needed to reach their goal, then rate sentences about the agent’s beliefs. In contrast with multimodal LLMs (GPT-4o, Gemini Pro) and ablated models, our model correlates highly with human judgments for a wide range of expressions, including modal language, uncertainty expressions, knowledge claims, likelihood comparisons, and attributions of false belief.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">ying2025understanding</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Understanding Epistemic Language with a Language-augmented Bayesian Theory of Mind}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ying, Lance and Zhi-Xuan, Tan and Wong, Lionel and Mansinghka, Vikash and Tenenbaum, Joshua B}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Transactions of the Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="kim2025hypothesis" class="col-sm-8"> <div class="title">Hypothesis-driven theory-of-mind reasoning for large language models</div> <div class="author"> Hyunwoo Kim, Melanie Sclar, <em>Tan Zhi-Xuan</em>, Lance Ying, Sydney Levine, Yang Liu, Joshua B Tenenbaum, and Yejin Choi</div> <div class="periodical"> <em>arXiv preprint arXiv:2502.11881</em> 2025 </div> <div class="links"> <a href="http://arxiv.org/abs/2502.11881" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2502.11881" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">kim2025hypothesis</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Hypothesis-driven theory-of-mind reasoning for large language models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kim, Hyunwoo and Sclar, Melanie and Zhi-Xuan, Tan and Ying, Lance and Levine, Sydney and Liu, Yang and Tenenbaum, Joshua B and Choi, Yejin}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2502.11881}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#008080"><a href="https://onlinelibrary.wiley.com/journal/17568765" rel="external nofollow noopener" target="_blank">TopiCS</a></abbr></div> <div id="trujillo2025resource" class="col-sm-8"> <div class="title">Resource-Rational Virtual Bargaining for Moral Judgment: Toward a Probabilistic Cognitive Model</div> <div class="author"> Diego Trujillo, Mindy Zhang, <em>Tan Zhi-Xuan</em>, Joshua B Tenenbaum, and Sydney Levine</div> <div class="periodical"> <em>Topics in Cognitive Science</em> 2025 </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://onlinelibrary.wiley.com/doi/full/10.1111/tops.12781" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://onlinelibrary.wiley.com/doi/pdfdirect/10.1111/tops.12781" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">trujillo2025resource</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Resource-Rational Virtual Bargaining for Moral Judgment: Toward a Probabilistic Cognitive Model}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Trujillo, Diego and Zhang, Mindy and Zhi-Xuan, Tan and Tenenbaum, Joshua B and Levine, Sydney}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Topics in Cognitive Science}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Wiley Online Library}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="year">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#00AFBD"><a href="https://link.springer.com/journal/11098" rel="external nofollow noopener" target="_blank">Phil.Studies</a></abbr></div> <div id="zhixuan2024beyond" class="col-sm-8"> <div class="title">Beyond Preferences in AI Alignment</div> <div class="author"> <em>Tan Zhi-Xuan</em>, Micah Carroll, Matija Franklin, and Hal Ashton</div> <div class="periodical"> <em>Philosophical Studies</em> Nov 2024 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2408.16984" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://link.springer.com/article/10.1007/s11098-024-02249-w" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://arxiv.org/pdf/2408.16984" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>The dominant practice of AI alignment assumes (1) that preferences are an adequate representation of human values, (2) that human rationality can be understood in terms of maximizing the satisfaction of preferences, and (3) that AI systems should be aligned with the preferences of one or more humans to ensure that they behave safely and in accordance with our values. Whether implicitly followed or explicitly endorsed, these commitments constitute what we term a preferentist approach to AI alignment. In this paper, we characterize and challenge the preferentist approach, describing conceptual and technical alternatives that are ripe for further research. We first survey the limits of rational choice theory as a descriptive model, explaining how preferences fail to capture the thick semantic content of human values, and how utility representations neglect the possible incommensurability of those values. We then critique the normativity of expected utility theory (EUT) for humans and AI, drawing upon arguments showing how rational agents need not comply with EUT, while highlighting how EUT is silent on which preferences are normatively acceptable. Finally, we argue that these limitations motivate a reframing of the targets of AI alignment: Instead of alignment with the preferences of a human user, developer, or humanity-writ-large, AI systems should be aligned with normative standards appropriate to their social roles, such as the role of a general-purpose assistant. Furthermore, these standards should be negotiated and agreed upon by all relevant stakeholders. On this alternative conception of alignment, a multiplicity of AI systems will be able to serve diverse ends, aligned with normative standards that promote mutual benefit and limit harm despite our plural and divergent values.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">zhixuan2024beyond</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Beyond Preferences in AI Alignment}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhi-Xuan, Tan and Carroll, Micah and Franklin, Matija and Ashton, Hal}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Philosophical Studies}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">nov</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#000000"><a href="https://www.nature.com/nathumbehav/" rel="external nofollow noopener" target="_blank">Nat.Hum.Behav.</a></abbr></div> <div id="collins2024building" class="col-sm-8"> <div class="title">Building Machines that Learn and Think with People</div> <div class="author"> Katherine M Collins, Ilia Sucholutsky, Umang Bhatt, Kartik Chandra, Lionel Wong, Mina Lee, Cedegao E Zhang, <em>Tan Zhi-Xuan</em>, Mark Ho, Vikash Mansinghka, Adrian Weller, Joshua B Tenenbaum, and Thomas L Griffiths</div> <div class="periodical"> <em>Nature Human Behaviour</em> Nov 2024 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2408.03943" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.nature.com/articles/s41562-024-01991-9" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://arxiv.org/pdf/2408.03943" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>What do we want from machine intelligence? We envision machines that are not just tools for thought, but partners in thought: reasonable, insightful, knowledgeable, reliable, and trustworthy systems that think with us. Current artificial intelligence (AI) systems satisfy some of these criteria, some of the time. In this Perspective, we show how the science of collaborative cognition can be put to work to engineer systems that really can be called “thought partners,” systems built to meet our expectations and complement our limitations. We lay out several modes of collaborative thought in which humans and AI thought partners can engage and propose desiderata for human-compatible thought partnerships. Drawing on motifs from computational cognitive science, we motivate an alternative scaling path for the design of thought partners and ecosystems around their use through a Bayesian lens, whereby the partners we construct actively build and reason over models of the human and world.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">collins2024building</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Building Machines that Learn and Think with People}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Collins, Katherine M and Sucholutsky, Ilia and Bhatt, Umang and Chandra, Kartik and Wong, Lionel and Lee, Mina and Zhang, Cedegao E and Zhi-Xuan, Tan and Ho, Mark and Mansinghka, Vikash and Weller, Adrian and Tenenbaum, Joshua B and Griffiths, Thomas L}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Nature Human Behaviour}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{8}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{10}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1851--1863}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#B509AC"><a href="https://cognitivesciencesociety.org/" rel="external nofollow noopener" target="_blank">CogSci</a></abbr></div> <div id="zhixuan2024infinite" class="col-sm-8"> <div class="title">Infinite Ends from Finite Samples: Open-Ended Goal Inference as Top-Down Bayesian Filtering of Bottom-Up Proposals</div> <div class="author"> <em>Tan Zhi-Xuan</em>, Gloria Kang, Vikash Mansinghka, and Joshua B Tenenbaum</div> <div class="periodical"> <em>Proceedings of the Annual Meeting of the Cognitive Science Society</em> Jul 2024 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2407.16770" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/2024-open-ended-goal-inference.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://osf.io/bygwm/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Supp</a> <a href="https://github.com/ztangent/OpenEndedBlockWords.jl" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>The space of human goals is tremendously vast; and yet, from just a few moments of watching a scene or reading a story, we seem to spontaneously infer a range of plausible motivations for the people and characters involved. What explains this remarkable capacity for intuiting other agents’ goals, despite the infinitude of ends they might pursue? And how does this cohere with our understanding of other people as approximately rational agents? In this paper, we introduce a sequential Monte Carlo model of open-ended goal inference, which combines top-down Bayesian inverse planning with bottom-up sampling based on the statistics of co-occurring subgoals. By proposing goal hypotheses related to the subgoals achieved by an agent, our model rapidly generates plausible goals without exhaustive search, then filters out goals that would be irrational given the actions taken so far. We validate this model in a goal inference task called Block Words, where participants try to guess the word that someone is stacking out of lettered blocks. In comparison to both heuristic bottom-up guessing and exact Bayesian inference over hundreds of goals, our model better predicts the mean, variance, efficiency, and resource rationality of human goal inferences, achieving similar accuracy to the exact model at a fraction of the cognitive cost, while also explaining garden-path effects that arise from misleading bottom-up cues. Our experiments thus highlight the importance of uniting top-down and bottom-up models for explaining the speed, accuracy, and generality of human theory-of-mind.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">zhixuan2024infinite</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Infinite Ends from Finite Samples: Open-Ended Goal Inference as Top-Down Bayesian Filtering of Bottom-Up Proposals}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhi-Xuan, Tan and Kang, Gloria and Mansinghka, Vikash and Tenenbaum, Joshua B}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Proceedings of the Annual Meeting of the Cognitive Science Society}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{46}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{46}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#B509AC"><a href="https://cognitivesciencesociety.org/" rel="external nofollow noopener" target="_blank">CogSci</a></abbr></div> <div id="ying2024grounding" class="col-sm-8"> <div class="title">Grounding Language about Belief in a Bayesian Theory-of-Mind</div> <div class="author"> Lance Ying, <em>Tan Zhi-Xuan</em>, Lionel Wong, Vikash Mansinghka, and Joshua Tenenbaum</div> <div class="periodical"> <em>Proceedings of the Annual Meeting of the Cognitive Science Society</em> Jul 2024 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2402.10416" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://escholarship.org/uc/item/2tk4d5jw" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://escholarship.org/content/qt2tk4d5jw/qt2tk4d5jw.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://osf.io/z7nvu/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Supp</a> </div> <div class="abstract hidden"> <p>Despite the fact that beliefs are mental states that cannot be directly observed, humans talk about each others’ beliefs on a regular basis, often using rich compositional language to describe what others think and know. What explains this capacity to interpret the hidden epistemic content of other minds? In this paper, we take a step towards an answer by grounding the semantics of belief statements in a Bayesian theory-of-mind: By modeling how humans jointly infer coherent sets of goals, beliefs, and plans that explain an agent’s actions, then evaluating statements about the agent’s beliefs against these inferences via epistemic logic, our framework provides a functional role semantics for belief, explaining the gradedness and compositionality of human belief attributions, as well as their intimate connection with goals and plans. We evaluate this framework by studying how humans attribute goals and evaluate belief sentences while watching an agent solve a doors-and-keys gridworld puzzle that requires instrumental reasoning about hidden objects. In contrast to pure logical deduction, non-mentalizing baselines, and mentalizing that ignores the role of instrumental plans, our model provides a much better fit to human goal and belief attributions, demonstrating the importance of theory-of-mind for modeling how humans understand language about beliefs.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">ying2024grounding</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Grounding Language about Belief in a Bayesian Theory-of-Mind}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ying, Lance and Zhi-Xuan, Tan and Wong, Lionel and Mansinghka, Vikash and Tenenbaum, Joshua}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Proceedings of the Annual Meeting of the Cognitive Science Society}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{46}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{46}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="dalrymple2024guaranteed" class="col-sm-8"> <div class="title">Towards Guaranteed Safe AI: A Framework for Ensuring Robust and Reliable AI Systems</div> <div class="author"> David Dalrymple, Joar Skalse, Yoshua Bengio, Stuart Russell, Max Tegmark, Sanjit Seshia, Steve Omohundro, Christian Szegedy, Ben Goldhaber, Nora Ammann, Alessandro Abate, Joe Halpern, Clark Barrett, Ding Zhao, <em>Tan Zhi-Xuan</em>, Jeannette Wing, and Joshua Tenenbaum</div> <div class="periodical"> <em>arXiv preprint arXiv:2405.06624</em> May 2024 </div> <div class="links"> <a href="http://arxiv.org/abs/2405.06624" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2405.06624.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">dalrymple2024guaranteed</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Towards Guaranteed Safe AI: A Framework for Ensuring Robust and Reliable AI Systems}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{"davidad" Dalrymple, David and Skalse, Joar and Bengio, Yoshua and Russell, Stuart and Tegmark, Max and Seshia, Sanjit and Omohundro, Steve and Szegedy, Christian and Goldhaber, Ben and Ammann, Nora and Abate, Alessandro and Halpern, Joe and Barrett, Clark and Zhao, Ding and Zhi-Xuan, Tan and Wing, Jeannette and Tenenbaum, Joshua}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2405.06624}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#1E88E5"><a href="https://dl.acm.org/conference/aamas" rel="external nofollow noopener" target="_blank">AAMAS</a></abbr></div> <div id="zhixuan2024pragmatic" class="col-sm-8"> <div class="title">Pragmatic Instruction Following and Goal Assistance via Cooperative Language Guided Inverse Plan Search</div> <div class="author"> <em>Tan Zhi-Xuan</em>, Lance Ying, Vikash Mansinghka, and Joshua B Tenenbaum</div> <div class="periodical"> <em>In Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems</em> May 2024 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2402.17930" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://dl.acm.org/doi/abs/10.5555/3635637.3663074" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/2024-clips.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://osf.io/v8ru7/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Supp</a> <a href="https://github.com/probcomp/CLIPS.jl" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>People often give instructions whose meaning is ambiguous without further context, expecting that their actions or goals will disambiguate their intentions. How can we build assistive agents that follow such instructions in a flexible, context-sensitive manner? This paper introduces cooperative language-guided inverse plan search (CLIPS), a Bayesian agent architecture for pragmatic instruction following and goal assistance. Our agent assists a human by modeling them as a cooperative planner who communicates joint plans to the assistant, then performs multimodal Bayesian inference over the human’s goal from actions and language, using large language models (LLMs) to evaluate the likelihood of an instruction given a hypothesized plan. Given this posterior, our assistant acts to minimize expected goal achievement cost, enabling it to pragmatically follow ambiguous instructions and provide effective assistance even when uncertain about the goal. We evaluate these capabilities in two cooperative planning domains (Doors, Keys &amp; Gems and VirtualHome), finding that CLIPS significantly outperforms GPT-4V, LLM-based literal instruction following and unimodal inverse planning in both accuracy and helpfulness, while closely matching the inferences and assistive judgments provided by human raters.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zhixuan2024pragmatic</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Pragmatic Instruction Following and Goal Assistance via Cooperative Language Guided Inverse Plan Search}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhi-Xuan, Tan and Ying, Lance and Mansinghka, Vikash and Tenenbaum, Joshua B}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#1E88E5"><a href="https://dl.acm.org/conference/aamas" rel="external nofollow noopener" target="_blank">AAMAS</a></abbr></div> <div id="oldenburg2024learning" class="col-sm-8"> <div class="title">Learning and Sustaining Shared Normative Systems via Bayesian Rule Induction in Markov Games</div> <div class="author"> Ninell Oldenburg, and <em>Tan Zhi-Xuan</em> </div> <div class="periodical"> <em>In Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems</em> May 2024 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2402.13399" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://dl.acm.org/doi/10.5555/3635637.3663011" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/2024-norm-learning-markov-games.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/ninell-oldenburg/social-contracts" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>A universal feature of human societies is the adoption of systems of rules and norms in the service of cooperative ends. How can we build learning agents that do the same, so that they may flexibly cooperate with the human institutions they are embedded in? We hypothesize that agents can achieve this by assuming there exists a shared set of norms that most others comply with while pursuing their individual desires, even if they do not know the exact content of those norms. By assuming shared norms, a newly introduced agent can infer the norms of an existing population from observations of compliance and violation. Furthermore, groups of agents can converge to a shared set of norms, even if they initially diverge in their beliefs about what the norms are. This in turn enables the stability of the normative system: since agents can bootstrap common knowledge of the norms, this leads the norms to be widely adhered to, enabling new entrants to rapidly learn those norms. We formalize this framework in the context of Markov games and demonstrate its operation in a multi-agent environment via approximately Bayesian rule induction of obligative and prohibitive norms. Using our approach, agents are able to rapidly learn and sustain a variety of cooperative institutions, including resource management norms and compensation for pro-social labor, promoting collective welfare while still allowing agents to act in their own interests.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">oldenburg2024learning</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Learning and Sustaining Shared Normative Systems via Bayesian Rule Induction in Markov Games}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Oldenburg, Ninell and Zhi-Xuan, Tan}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="year">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="lew2023sequential" class="col-sm-8"> <div class="title">Sequential Monte Carlo Steering of Large Language Models using Probabilistic Programs</div> <div class="author"> Alexander K Lew, <em>Tan Zhi-Xuan</em>, Gabriel Grand, and Vikash K Mansinghka</div> <div class="periodical"> <em>In ICML 2023 Workshop on Sampling and Optimization in Discrete Spaces (SoDS)</em> Jul 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2306.03081" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2306.03081.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Even after fine-tuning and reinforcement learning, large language models (LLMs) can be difficult, if not impossible, to control reliably with prompts alone. We propose a new inference-time approach to enforcing syntactic and semantic constraints on the outputs of LLMs, called sequential Monte Carlo (SMC) steering. The key idea is to specify language generation tasks as posterior inference problems in a class of discrete probabilistic sequence models, and replace standard decoding with sequential Monte Carlo inference. For a computational cost similar to that of beam search, SMC can steer LLMs to solve diverse tasks, including infilling, generation under syntactic constraints, and prompt intersection. To facilitate experimentation with SMC steering, we present a probabilistic programming library, LLaMPPL (this https URL), for concisely specifying new generation tasks as language model probabilistic programs, and automating steering of LLaMA-family Transformers.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">lew2023sequential</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Sequential Monte Carlo Steering of Large Language Models using Probabilistic Programs}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lew, Alexander K and Zhi-Xuan, Tan and Grand, Gabriel and Mansinghka, Vikash K}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ICML 2023 Workshop on Sampling and Optimization in Discrete Spaces (SoDS)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="ying2023inferring" class="col-sm-8"> <div class="title">Inferring the goals of communicating agents from actions and instructions</div> <div class="author"> Lance Ying, <em>Tan Zhi-Xuan</em>, Vikash Mansinghka, and Joshua B Tenenbaum</div> <div class="periodical"> <em>In Proceedings of the 2023 AAAI Fall Symposia</em> Jul 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2306.16207" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ojs.aaai.org/index.php/AAAI-SS/article/view/27645" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://arxiv.org/pdf/2306.16207.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://osf.io/gh758/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Supp</a> </div> <div class="abstract hidden"> <p>When humans cooperate, they frequently coordinate their activity through both verbal communication and non-verbal actions, using this information to infer a shared goal and plan. How can we model this inferential ability? In this paper, we introduce a model of a cooperative team where one agent, the principal, may communicate natural language instructions about their shared plan to another agent, the assistant, using GPT-3 as a likelihood function for instruction utterances. We then show how a third person observer can infer the team’s goal via multi-modal Bayesian inverse planning from actions and instructions, computing the posterior distribution over goals under the assumption that agents will act and communicate rationally to achieve them. We evaluate this approach by comparing it with human goal inferences in a multi-agent gridworld, finding that our model’s inferences closely correlate with human judgments (R = 0.96). When compared to inference from actions alone, we also find that instructions lead to more rapid and less uncertain goal inference, highlighting the importance of verbal communication for cooperative agents.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">ying2023inferring</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Inferring the goals of communicating agents from actions and instructions}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ying, Lance and Zhi-Xuan, Tan and Mansinghka, Vikash and Tenenbaum, Joshua B}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 2023 AAAI Fall Symposia}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{2}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{1}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{26--33}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="ying2023neuro" class="col-sm-8"> <div class="title">The Neuro-Symbolic Inverse Planning Engine (NIPE): Modeling Probabilistic Social Inferences from Linguistic Inputs</div> <div class="author"> Lance Ying, Katherine M Collins, Megan Wei, Cedegao E Zhang, <em>Tan Zhi-Xuan</em>, Adrian Weller, Joshua B Tenenbaum, and Lionel Wong</div> <div class="periodical"> <em>In ICML 2023 Workshop on Theory of Mind in Communicating Agents</em> Jul 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2306.14325" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2306.14325.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Human beings are social creatures. We routinely reason about other agents, and a crucial component of this social reasoning is inferring people’s goals as we learn about their actions. In many settings, we can perform intuitive but reliable goal inference from language descriptions of agents, actions, and the background environments. In this paper, we study this process of language driving and influencing social reasoning in a probabilistic goal inference domain. We propose a neuro-symbolic model that carries out goal inference from linguistic inputs of agent scenarios. The "neuro" part is a large language model (LLM) that translates language descriptions to code representations, and the "symbolic" part is a Bayesian inverse planning engine. To test our model, we design and run a human experiment on a linguistic goal inference task. Our model closely matches human response patterns and better predicts human judgements than using an LLM alone.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">ying2023neuro</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{The Neuro-Symbolic Inverse Planning Engine (NIPE): Modeling Probabilistic Social Inferences from Linguistic Inputs}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ying, Lance and Collins, Katherine M and Wei, Megan and Zhang, Cedegao E and Zhi-Xuan, Tan and Weller, Adrian and Tenenbaum, Joshua B and Wong, Lionel}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ICML 2023 Workshop on Theory of Mind in Communicating Agents}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#B509AC"><a href="https://cognitivesciencesociety.org/" rel="external nofollow noopener" target="_blank">CogSci</a></abbr></div> <div id="kwon2023not" class="col-sm-8"> <div class="title">When it is not out of line to get out of line: The role of universalization and outcome-based reasoning in rule-breaking judgments</div> <div class="author"> Joe Kwon, <em>Tan Zhi-Xuan</em>, Joshua Tenenbaum, and Sydney Levine</div> <div class="periodical"> <em>In Proceedings of the Annual Meeting of the Cognitive Science Society</em> Jul 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/2023-universalization-rule-breaking.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>How do we know when it’s OK to break moral rules? We propose that — alongside well-studied outcome-based measures of welfare and harm — people sometimes use universalization, asking "What if everyone felt at liberty to ignore the rule?" We develop a virtual environment where agents stand in line to gather water. Subjects judge agents who get out of line to try to get water more quickly. If subjects use universalization, they would need to imagine all agents getting out of line and going straight for the water in each environment. To test this prediction, we model an action’s universalizability by simulating what would happen if every agent tried to follow a path directly to the water, then evaluating the effects. We also investigate the role of several outcome-based measures, including welfare aggregation and harm-based measures. We find that universalizability plays an important role in rule-breaking judgments alongside outcome-based concerns.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">kwon2023not</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{When it is not out of line to get out of line: The role of universalization and outcome-based reasoning in rule-breaking judgments}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kwon, Joe and Zhi-Xuan, Tan and Tenenbaum, Joshua and Levine, Sydney}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the Annual Meeting of the Cognitive Science Society}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{45}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{45}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="zhixuan2023bayesian" class="col-sm-8"> <div class="title">Bayesian Inverse Motion Planning for Online Goal Inference in Continuous Domains</div> <div class="author"> <em>Tan Zhi-Xuan</em>, Jovana Kondic, Stewart Slocum, Joshua B Tenenbaum, Vikash K Mansinghka, and Dylan Hadfield-Menell</div> <div class="periodical"> <em>In ICRA 2023 Workshop on Cognitive Modeling in Robot Learning</em> Jun 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://sites.google.com/view/cognitive-modeling-icra2023-ws/contributions?authuser=0#h.dk14d3kbwe65" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/2023-inverse-motion-planning.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/InverseMotionPlanning/InverseMotionPlanning.jl" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Humans and other agents navigate their environments by acting efficiently to achieve their goals. In order to infer agents’ goals from their actions, it is thus necessary to model how agents achieve their goals efficiently. Here, we show how online goal inference and trajectory prediction in continuous domains can be performed via Bayesian inverse motion planning: By modeling an agent as an approximately Boltzmann-rational motion planner that produces low-cost trajectories while avoiding obstacles, and placing a prior over goals, we can infer the agent’s goal and future trajectory from partial trajectory observations. We compute these inferences online using a sequential Monte Carlo algorithm, which accounts for the multimodal distribution of trajectories due to obstacles, and exhibits better calibration at early timesteps than a Laplace approximation and a greedy baseline.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zhixuan2023bayesian</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Bayesian Inverse Motion Planning for Online Goal Inference in Continuous Domains}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhi-Xuan, Tan and Kondic, Jovana and Slocum, Stewart and Tenenbaum, Joshua B and Mansinghka, Vikash K and Hadfield-Menell, Dylan}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ICRA 2023 Workshop on Cognitive Modeling in Robot Learning}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jun</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://sites.google.com/view/cognitive-modeling-icra2023-ws/contributions?authuser=0#h.dk14d3kbwe65}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#009E73"><a href="https://aistats.org/" rel="external nofollow noopener" target="_blank">AISTATS</a></abbr></div> <div id="lew2023smcp3" class="col-sm-8"> <div class="title">SMCP3: Sequential Monte Carlo with Probabilistic Program Proposals</div> <div class="author"> Alexander K. Lew, George Matheos, <em>Tan Zhi-Xuan</em>, Matin Ghavamizadeh, Nishad Gothoskar, Stuart Russell, and Vikash K. Mansinghka</div> <div class="periodical"> <em>In Proceedings of The 26th International Conference on Artificial Intelligence and Statistics</em> Apr 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://proceedings.mlr.press/v206/lew23a.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://proceedings.mlr.press/v206/lew23a/lew23a.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>This paper introduces SMCP3, a method for automatically implementing custom sequential Monte Carlo samplers for inference in probabilistic programs. Unlike particle filters and resample-move SMC (Gilks and Berzuini, 2001), SMCP3 algorithms can improve the quality of samples and weights using pairs of Markov proposal kernels that are also specified by probabilistic programs. Unlike Del Moral et al. (2006b), these proposals can themselves be complex probabilistic computations that generate auxiliary variables, apply deterministic transformations, and lack tractable marginal densities. This paper also contributes an efficient implementation in Gen that eliminates the need to manually derive incremental importance weights. SMCP3 thus simultaneously expands the design space that can be explored by SMC practitioners and reduces the implementation effort. SMCP3 is illustrated using applications to 3D object tracking, state-space modeling, and data clustering, showing that SMCP3 methods can simultaneously improve the quality and reduce the cost of marginal likelihood estimation and posterior inference.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">lew2023smcp3</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{SMCP3: Sequential Monte Carlo with Probabilistic Program Proposals}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lew, Alexander K. and Matheos, George and Zhi-Xuan, Tan and Ghavamizadeh, Matin and Gothoskar, Nishad and Russell, Stuart and Mansinghka, Vikash K.}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of The 26th International Conference on Artificial Intelligence and Statistics}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{7061--7088}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Ruiz, Francisco and Dy, Jennifer and van de Meent, Jan-Willem}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{206}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{Proceedings of Machine Learning Research}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">apr</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{PMLR}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="year">2022</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="zhixuan2022abstract" class="col-sm-8"> <div class="title">Abstract Interpretation for Generalized Heuristic Search in Model-Based Planning</div> <div class="author"> <em>Tan Zhi-Xuan</em>, Joshua B Tenenbaum, and Vikash K Mansinghka</div> <div class="periodical"> <em>In ICML 2022 Workshop on Beyond Bayes: Paths Towards Universal Reasoning Systems</em> Apr 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2208.02938" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/2022-absint-planning.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Domain-general model-based planners often derive their generality by constructing search heuristics through the relaxation or abstraction of symbolic world models. We illustrate how abstract interpretation can serve as a unifying framework for these abstraction-based heuristics, extending the reach of heuristic search to richer world models that make use of more complex datatypes and functions (e.g. sets, geometry), and even models with uncertainty and probabilistic effects. These heuristics can also be integrated with learning, allowing agents to jumpstart planning in novel world models via abstraction-derived information that is later refined by experience. This suggests that abstract interpretation can play a key role in building universal reasoning systems.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex">  <span class="c">title = {Abstract Interpretation for Generalized Heuristic Search in Model-Based Planning},</span>
  <span class="c">author = {Zhi-Xuan, Tan and Tenenbaum, Joshua B and Mansinghka, Vikash K},</span>
  <span class="c">booktitle = {ICML 2022 Workshop on Beyond Bayes: Paths Towards Universal Reasoning Systems},</span>
  <span class="c">year = {2022},</span>
<span class="c">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="zhixuan2022solving" class="col-sm-8"> <div class="title">Solving the Baby Intuitions Benchmark with a Hierarchically Bayesian Theory of Mind</div> <div class="author"> <em>Tan Zhi-Xuan</em>, Nishad Gothoskar, Falk Pollok, Dan Gutfreund, Joshua B Tenenbaum, and Vikash K Mansinghka</div> <div class="periodical"> <em>In RSS 2022 Workshop on Social Intelligence in Humans and Robots</em> Apr 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2208.02914" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/2022-solving-bib.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>To facilitate the development of new models to bridge the gap between machine and human social intelligence, the recently proposed Baby Intuitions Benchmark (arXiv:2102.11938) provides a suite of tasks designed to evaluate commonsense reasoning about agents’ goals and actions that even young infants exhibit. Here we present a principled Bayesian solution to this benchmark, based on a hierarchically Bayesian Theory of Mind (HBToM). By including hierarchical priors on agent goals and dispositions, inference over our HBToM model enables few-shot learning of the efficiency and preferences of an agent, which can then be used in commonsense plausibility judgements about subsequent agent behavior. This approach achieves near-perfect accuracy on most benchmark tasks, outperforming deep learning and imitation learning baselines while producing interpretable human-like inferences, demonstrating the advantages of structured Bayesian models of human social cognition.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zhixuan2022solving</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Solving the Baby Intuitions Benchmark with a Hierarchically Bayesian Theory of Mind}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhi-Xuan, Tan and Gothoskar, Nishad and Pollok, Falk and Gutfreund, Dan and Tenenbaum, Joshua B and Mansinghka, Vikash K}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{RSS 2022 Workshop on Social Intelligence in Humans and Robots}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="zhixuan2022pddl" class="col-sm-8"> <div class="title">PDDL. jl: An Extensible Interpreter and Compiler Interface for Fast and Flexible AI Planning</div> <div class="author"> <em>Tan Zhi-Xuan</em> </div> <div class="periodical"> Apr 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://dspace.mit.edu/handle/1721.1/143179" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://dspace.mit.edu/bitstream/handle/1721.1/143179/Tan-xuan-SM-EECS-2022-thesis.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/JuliaPlanners/PDDL.jl" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>The Planning Domain Definition Language (PDDL) is a formal specification language for symbolic planning problems and domains that is widely used by the AI planning community. However, most implementations of PDDL are closely tied to particular planning systems and algorithms, and are not designed for interoperability or modular use within larger AI systems. This limitation also makes it difficult to support extensions to PDDL without implementing a dedicated planner for that extension, inhibiting the generality and reach of automated planning. To address these limitations, we present PDDL.jl, an extensible interpreter and compiler interface for fast and flexible AI planning. PDDL.jl exposes the semantics of planning domains through a common interface for executing actions, querying state variables, and other basic operations used within AI planning applications. PDDL.jl also supports the extension of PDDL semantics (e.g. to stochastic and continuous domains), domain abstraction for generalized heuristic search (via abstract interpretation), and domain compilation for efficient planning, enabling speed and flexibility for PDDL and its many descendants. Collectively, these features allow PDDL.jl to serve as a general high-performance platform for AI applications and research programs that leverage the integration of symbolic planning with other AI technologies, such as neuro-symbolic reinforcement learning, probabilistic programming, and Bayesian inverse planning for value learning and goal inference.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@phdthesis</span><span class="p">{</span><span class="nl">zhixuan2022pddl</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{PDDL. jl: An Extensible Interpreter and Compiler Interface for Fast and Flexible AI Planning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhi-Xuan, Tan}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">school</span> <span class="p">=</span> <span class="s">{Massachusetts Institute of Technology}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://dspace.mit.edu/handle/1721.1/143179}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="year">2021</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#B509AC"><a href="https://cognitivesciencesociety.org/" rel="external nofollow noopener" target="_blank">CogSci</a></abbr></div> <div id="alanqary2021modeling" class="col-sm-8"> <div class="title">Modeling the Mistakes of Boundedly Rational Agents Within a Bayesian Theory of Mind</div> <div class="author"> Arwa Alanqary, Gloria Z Lin, Joie Le, <em>Tan Zhi-Xuan</em>, Vikash K Mansinghka, and Joshua B Tenenbaum</div> <div class="periodical"> <em>In Proceedings of the Annual Meeting of the Cognitive Science Society</em> Jul 2021 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2106.13249" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://escholarship.org/uc/item/7tr2w3c9" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://arxiv.org/pdf/2106.13249.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>When inferring the goals that others are trying to achieve, people intuitively understand that others might make mistakes along the way. This is crucial for activities such as teaching, offering assistance, and deciding between blame or forgiveness. However, Bayesian models of theory of mind have generally not accounted for these mistakes, instead modeling agents as mostly optimal in achieving their goals. As a result, they are unable to explain phenomena like locking oneself out of one’s house, or losing a game of chess. Here, we extend the Bayesian Theory of Mind framework to model boundedly rational agents who may have mistaken goals, plans, and actions. We formalize this by modeling agents as probabilistic programs, where goals may be confused with semantically similar states, plans may be misguided due to resource-bounded planning, and actions may be unintended due to execution errors. We present experiments eliciting human goal inferences in two domains: (i) a gridworld puzzle with gems locked behind doors, and (ii) a block-stacking domain. Our model better explains human inferences than alternatives, while generalizing across domains. These findings indicate the importance of modeling others as bounded agents, in order to account for the full richness of human intuitive psychology.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">alanqary2021modeling</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Modeling the Mistakes of Boundedly Rational Agents Within a Bayesian Theory of Mind}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Alanqary, Arwa and Lin, Gloria Z and Le, Joie and Zhi-Xuan, Tan and Mansinghka, Vikash K and Tenenbaum, Joshua B}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the Annual Meeting of the Cognitive Science Society}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{43}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{43}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="zhixuan2021genify" class="col-sm-8"> <div class="title">Genify.jl: Transforming Julia into Gen to enable programmable inference</div> <div class="author"> <em>Tan Zhi-Xuan</em>, McCoy R Becker, and Vikash K. Mansinghka</div> <div class="periodical"> <em>In Languages For Inference Workshop (LAFI 2021), 48th ACM SIGPLAN Symposium on Principles of Programming Languages</em> Jan 2021 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://popl21.sigplan.org/details/lafi-2021-papers/5/Genify-jl-Transforming-Julia-into-Gen-to-enable-programmable-inference" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/2021-genify.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/probcomp/Genify.jl" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>A wide variety of libraries written in Julia implement stochastic simulators of natural and social phenomena for the purposes of computational science. However, these simulators are not generally amenable to Bayesian inference, as they do not provide likelihoods for execution traces, support constraining of observed random variables, or allow random choices and subroutines to be selectively updated in Monte Carlo algorithms. To address these limitations, we present Genify.jl, an approach to transforming plain Julia code into generative functions in Gen, a universal probabilistic programming system with programmable inference. We accomplish this via lightweight transformation of lowered Julia code into Gen’s dynamic modeling language, combined with a user-friendly random variable addressing scheme that enables straightforward implementation of custom inference programs. We demonstrate the utility of this approach by transforming an existing agent-based simulator from plain Julia into Gen, and designing custom inference programs that increase accuracy and efficiency relative to generic SMC and MCMC methods. This performance improvement is achieved by proposing, constraining, or re-simulating random variables that are internal to the simulator, which is made possible by transformation into Gen.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zhixuan2021genify</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Genify.jl: Transforming Julia into Gen to enable programmable inference}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhi-Xuan, Tan and Becker, McCoy R and Mansinghka, Vikash K.}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Languages For Inference Workshop (LAFI 2021), 48th ACM SIGPLAN Symposium on Principles of Programming Languages}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jan</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="year">2020</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#00369F"><a href="https://neurips.cc/" rel="external nofollow noopener" target="_blank">NeurIPS</a></abbr></div> <div id="zhixuan2020online" class="col-sm-8"> <div class="title">Online Bayesian Goal Inference for Boundedly Rational Planning Agents</div> <div class="author"> <em>Tan Zhi-Xuan</em>, Jordyn Mann, Tom Silver, Josh Tenenbaum, and Vikash Mansinghka</div> <div class="periodical"> <em>In Proceedings of the 34th International Conference on Neural Information Processing Systems</em> Dec 2020 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2006.07532" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://proceedings.neurips.cc/paper/2020/hash/df3aebc649f9e3b674eeb790a4da224e-Abstract.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://proceedings.neurips.cc/paper/2020/file/df3aebc649f9e3b674eeb790a4da224e-Paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/probcomp/InversePlanning.jl" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>People routinely infer the goals of others by observing their actions over time. Remarkably, we can do so even when those actions lead to failure, enabling us to assist others when we detect that they might not achieve their goals. How might we endow machines with similar capabilities? Here we present an architecture capable of inferring an agent’s goals online from both optimal and non-optimal sequences of actions. Our architecture models agents as boundedly-rational planners that interleave search with execution by replanning, thereby accounting for sub-optimal behavior. These models are specified as probabilistic programs, allowing us to represent and perform efficient Bayesian inference over an agent’s goals and internal planning processes. To perform such inference, we develop Sequential Inverse Plan Search (SIPS), a sequential Monte Carlo algorithm that exploits the online replanning assumption of these models, limiting computation by incrementally extending inferred plans as new actions are observed. We present experiments showing that this modeling and inference architecture outperforms Bayesian inverse reinforcement learning baselines, accurately inferring goals from both optimal and non-optimal trajectories involving failure and back-tracking, while generalizing across domains with compositional structure and sparse rewards.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zhixuan2020online</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Online Bayesian Goal Inference for Boundedly Rational Planning Agents}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhi-Xuan, Tan and Mann, Jordyn and Silver, Tom and Tenenbaum, Josh and Mansinghka, Vikash}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 34th International Conference on Neural Information Processing Systems}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{19238--19250}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">dec</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://aaai.org/Conferences/AAAI/aaai.php" rel="external nofollow noopener" target="_blank">AAAI</a></abbr></div> <div id="zhixuan2020factorized" class="col-sm-8"> <div class="title">Factorized Inference in Deep Markov Models for Incomplete Multimodal Time Series</div> <div class="author"> <em>Tan Zhi-Xuan</em>, Harold Soh, and Desmond C Ong</div> <div class="periodical"> <em>In Proceedings of the AAAI Conference on Artificial Intelligence</em> Feb 2020 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/1905.13570" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ojs.aaai.org/index.php/AAAI/article/view/6597" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://ojs.aaai.org/index.php/AAAI/article/view/6597/6453" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/ztangent/multimodal-dmm" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Integrating deep learning with latent state space models has the potential to yield temporal models that are powerful, yet tractable and interpretable. Unfortunately, current models are not designed to handle missing data or multiple data modalities, which are both prevalent in real-world data. In this work, we introduce a factorized inference method for Multimodal Deep Markov Models (MDMMs), allowing us to filter and smooth in the presence of missing data, while also performing uncertainty-aware multimodal fusion. We derive this method by factorizing the posterior p (z| x) for non-linear state space models, and develop a variational backward-forward algorithm for inference. Because our method handles incompleteness over both time and modalities, it is capable of interpolation, extrapolation, conditional generation, label prediction, and weakly supervised learning of multimodal time series. We demonstrate these capabilities on both synthetic and real-world multimodal data under high levels of data deletion. Our method performs well even with more than 50% missing data, and outperforms existing deep approaches to inference in latent time series.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zhixuan2020factorized</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Factorized Inference in Deep Markov Models for Incomplete Multimodal Time Series}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhi-Xuan, Tan and Soh, Harold and Ong, Desmond C}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the AAAI Conference on Artificial Intelligence}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{34}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{06}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{10334--10341}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">feb</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1609/aaai.v34i06.6597}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="tan2020predator" class="col-sm-8"> <div class="title">Predator dormancy is a stable adaptive strategy due to Parrondo’s paradox</div> <div class="author"> <em>Zhi-Xuan Tan</em>, Jin Ming Koh, Eugene V Koonin, and Kang Hao Cheong</div> <div class="periodical"> <em>Advanced Science</em> Feb 2020 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://onlinelibrary.wiley.com/doi/full/10.1002/advs.201901559" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/advs.201901559" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Many predators produce dormant offspring to escape harsh environmental conditions, but the evolutionary stability of this adaptation has not been fully explored. Like seed banks in plants, dormancy provides a stable competitive advantage when seasonal variations occur, because the persistence of dormant forms under harsh conditions compensates for the increased cost of producing dormant offspring. However, dormancy also exists in environments with minimal abiotic variation—an observation not accounted for by existing theory. Here it is demonstrated that dormancy can out-compete perennial activity under conditions of extensive prey density fluctuation caused by overpredation. It is shown that at a critical level of prey density fluctuations, dormancy becomes an evolutionarily stable strategy. This is interpreted as a manifestation of Parrondo’s paradox: although neither the active nor dormant forms of a dormancy-capable predator can individually out-compete a perennially active predator, alternating between these two losing strategies can paradoxically result in a winning strategy. Parrondo’s paradox may thus explain the widespread success of quiescent behavioral strategies such as dormancy, suggesting that dormancy emerges as a natural evolutionary response to the self-destructive tendencies of overpredation and related biological phenomena.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">tan2020predator</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Predator dormancy is a stable adaptive strategy due to Parrondo's paradox}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tan, Zhi-Xuan and Koh, Jin Ming and Koonin, Eugene V and Cheong, Kang Hao}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Advanced Science}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{7}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{3}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1901559}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Wiley Online Library}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="year">2019</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="ong2019modeling" class="col-sm-8"> <div class="title">Modeling emotion in complex stories: the Stanford Emotional Narratives Dataset</div> <div class="author"> Desmond C Ong, Zhengxuan Wu, <em>Zhi-Xuan Tan</em>, Marianne Reddan, Isabella Kahhale, Alison Mattek, and Jamil Zaki</div> <div class="periodical"> <em>IEEE Transactions on Affective Computing</em> Feb 2019 </div> <div class="links"> <a href="http://arxiv.org/abs/1912.05008" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8414991/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">ong2019modeling</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Modeling emotion in complex stories: the Stanford Emotional Narratives Dataset}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ong, Desmond C and Wu, Zhengxuan and Tan, Zhi-Xuan and Reddan, Marianne and Kahhale, Isabella and Mattek, Alison and Zaki, Jamil}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Affective Computing}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{12}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{3}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{579--594}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="wu2019attending" class="col-sm-8"> <div class="title">Attending to emotional narratives</div> <div class="author"> Zhengxuan Wu, Xiyu Zhang, <em>Tan Zhi-Xuan</em>, Jamil Zaki, and Desmond C Ong</div> <div class="periodical"> <em>In 2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII)</em> Feb 2019 </div> <div class="links"> <a href="http://arxiv.org/abs/1907.04197" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/abstract/document/8925497" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">wu2019attending</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Attending to emotional narratives}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wu, Zhengxuan and Zhang, Xiyu and Zhi-Xuan, Tan and Zaki, Jamil and Ong, Desmond C}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{648--654}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#B509AC"><a href="https://cognitivesciencesociety.org/" rel="external nofollow noopener" target="_blank">CogSci</a></abbr></div> <div id="tan2019bayesian" class="col-sm-8"> <div class="title">Bayesian Inference of Social Norms as Shared Constraints on Behavior</div> <div class="author"> <em>Zhi-Xuan Tan</em>, and Desmond C Ong</div> <div class="periodical"> <em>In Proceedings of the Annual Meeting of the Cognitive Science Society</em> Jul 2019 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/1905.11110" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://cogsci.mindmodeling.org/2019/papers/0493/0493.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/ztangent/norms-cogsci19" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>People act upon their desires, but often, also act in adherence to implicit social norms. How do people infer these unstated social norms from others’ behavior, especially in novel social contexts? We propose that laypeople have intuitive theories of social norms as behavioral constraints shared across different agents in the same social context. We formalize inference of norms using a Bayesian Theory of Mind approach, and show that this computational approach provides excellent predictions of how people infer norms in two scenarios. Our results suggest that people separate the influence of norms and individual desires on others’ actions, and have implications for modelling generalizations of hidden causes of behavior.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">tan2019bayesian</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Bayesian Inference of Social Norms as Shared Constraints on Behavior}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tan, Zhi-Xuan and Ong, Desmond C}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the Annual Meeting of the Cognitive Science Society}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{41}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{41}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2919--2925}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://aaai.org/Conferences/AAAI/aaai.php" rel="external nofollow noopener" target="_blank">AAAI</a></abbr></div> <div id="tan2019thats" class="col-sm-8"> <div class="title">That’s mine! Learning ownership relations and norms for robots</div> <div class="author"> <em>Zhi-Xuan Tan</em>, Jake Brawer, and Brian Scassellati</div> <div class="periodical"> <em>In Proceedings of the AAAI Conference on Artificial Intelligence</em> Jan 2019 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/1812.02576" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ojs.aaai.org/index.php/AAAI/article/view/4808" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://ojs.aaai.org/index.php/AAAI/article/view/4808/4686" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>The ability for autonomous agents to learn and conform to human norms is crucial for their safety and effectiveness in social environments. While recent work has led to frameworks for the representation and inference of simple social rules, research into norm learning remains at an exploratory stage. Here, we present a robotic system capable of representing, learning, and inferring ownership relations and norms. Ownership is represented as a graph of probabilistic relations between objects and their owners, along with a database of predicate-based norms that constrain the actions permissible on owned objects. To learn these norms and relations, our system integrates (i) a novel incremental norm learning algorithm capable of both one-shot learning and induction from specific examples,(ii) Bayesian inference of ownership relations in response to apparent rule violations, and (iii) perceptbased prediction of an object’s likely owners. Through a series of simulated and real-world experiments, we demonstrate the competence and flexibility of the system in performing object manipulation tasks that require a variety of norms to be followed, laying the groundwork for future research into the acquisition and application of social norms.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">tan2019thats</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{That’s mine! Learning ownership relations and norms for robots}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tan, Zhi-Xuan and Brawer, Jake and Scassellati, Brian}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the AAAI Conference on Artificial Intelligence}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{33}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{01}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{8058--8065}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jan</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="year">2018</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="tan2018cross" class="col-sm-8"> <div class="title">Cross-issue solidarity and truth convergence in opinion dynamics</div> <div class="author"> <em>Zhi Xuan Tan</em>, and Kang Hao Cheong</div> <div class="periodical"> <em>Journal of Physics A: Mathematical and Theoretical</em> Jan 2018 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://iopscience.iop.org/article/10.1088/1751-8121/aad030/meta" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/2017-cross-issue-solidarity.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>How do movements and coalitions which engage with multiple social issues succeed in cross-issue solidarity, and when do they instead become fragmented? To address this, the mechanisms of cross-issue interaction have to be understood. Prior work on opinion dynamics and political disagreement has focused on single-issue consensus and polarization. Inspired by practices of cross-issue movement building, we have developed a general model of multi-issue opinion dynamics where agreement on one issue can promote greater inclusivity in discussing other issues, thereby avoiding the pitfalls of exclusivist interaction, where individuals engage only if they agree sufficiently on every issue considered. Our model shows that as more issues come into play, consensus and solidarity can only be maintained if inclusivity towards differing positions is increased. We further investigate whether greater inclusivity and compromise across issues lead people towards or away from normative truth, thereby addressing concerns about the non-ideal nature of political consensus.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">tan2018cross</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Cross-issue solidarity and truth convergence in opinion dynamics}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tan, Zhi Xuan and Cheong, Kang Hao}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Journal of Physics A: Mathematical and Theoretical}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{51}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{35}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{355101}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{IOP Publishing}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="year">2017</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="tan2017nomadic" class="col-sm-8"> <div class="title">Nomadic-colonial life strategies enable paradoxical survival and growth despite habitat destruction</div> <div class="author"> <em>Zhi Xuan Tan</em>, and Kang Hao Cheong</div> <div class="periodical"> <em>Elife</em> Jan 2017 </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://elifesciences.org/articles/21673" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">tan2017nomadic</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Nomadic-colonial life strategies enable paradoxical survival and growth despite habitat destruction}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tan, Zhi Xuan and Cheong, Kang Hao}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Elife}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{6}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2017}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{eLife Sciences Publications, Ltd}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.7554/eLife.21673}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Xuan (Tan Zhi Xuan). Powered by <a href="http://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>